# --- ModelTest.py (ì—…ë°ì´íŠ¸) ---

import pygame
import numpy as np
import torch
import Model
import ENV
import sys

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# ===== ìœ í‹¸: ë°°ìš° ìž…ë ¥ ì°¨ì›ì„ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¶”ë¡  =====
def infer_actor_input_dim_from_state_dict(sd: dict) -> int | None:
    # GaussianPolicyì˜ ì²« LinearëŠ” fc[0] ì´ê³  í‚¤ëŠ” 'fc.0.weight'ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
    # ëª¨ë“ˆëª…ì´ ë°”ë€Œì—ˆì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ëª¨ë“  Linear weightì—ì„œ in_features í›„ë³´ë¥¼ ì°¾ìŒ
    for k, v in sd.items():
        if isinstance(v, torch.Tensor) and v.ndim == 2:  # [out_features, in_features]
            # ì²« ë ˆì´ì–´ í›„ë³´: ë³´í†µ ê°€ìž¥ ìž‘ì€ in_features(ìž…ë ¥ ì°¨ì›)ì´ê±°ë‚˜ ì´ë¦„ìƒ fc.0.weight
            if "fc.0.weight" in k:
                return v.shape[1]
    # fc.0.weightê°€ ì—†ë‹¤ë©´ ì „ì²´ ì¤‘ ê°€ìž¥ ìž‘ì€ in_featuresë¥¼ ìž…ë ¥ì°¨ì›ìœ¼ë¡œ ì¶”ì •
    candidates = [v.shape[1] for k, v in sd.items() if isinstance(v, torch.Tensor) and v.ndim == 2]
    return min(candidates) if candidates else None


# ===== ìœ í‹¸: ë°°ìš° ëª¨ë“ˆì—ì„œ ì²« Linearì˜ in_features ì½ê¸° =====
def actor_input_dim_from_module(actor: torch.nn.Module) -> int:
    for m in actor.modules():
        if isinstance(m, torch.nn.Linear):
            return m.in_features
    raise RuntimeError("Actor has no Linear layer to infer input dim.")


# ===== ìœ í‹¸: ìƒíƒœë¥¼ ë°°ìš°ê°€ ê¸°ëŒ€í•˜ëŠ” ìž…ë ¥ ì°¨ì›ì— ë§žê²Œ ì¡°ì • =====
def adapt_state_for_actor(state_np: np.ndarray, expected_dim: int) -> np.ndarray:
    cur = state_np.shape[0]
    if cur == expected_dim:
        return state_np
    elif cur > expected_dim:
        # ì•žìª½ í”¼ì²˜ë¥¼ ìš°ì„  ì‚¬ìš© (agent, goal ë¨¼ì €ì´ë¯€ë¡œ ì˜ë¯¸ ë³´ì¡´)
        return state_np[:expected_dim]
    else:
        # ëª¨ìžë¼ë©´ 0 íŒ¨ë”©
        pad = np.zeros(expected_dim - cur, dtype=state_np.dtype)
        return np.concatenate([state_np, pad], axis=0)


def evaluate(env, actor, scale=2.3, wait=10, auto_quit=True):
    pygame.init()
    size = (600, 600)
    screen = pygame.display.set_mode(size)
    pygame.display.set_caption("SAC Agent Navigation (Real-Time)")
    clock = pygame.time.Clock()

    actor.eval()
    running = True

    state, _ = env.reset()
    # ë°°ìš°ê°€ ì‹¤ì œ ê¸°ëŒ€í•˜ëŠ” ìž…ë ¥ ì°¨ì›
    expected_dim = actor_input_dim_from_module(actor)
    state = adapt_state_for_actor(state, expected_dim)
    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)

    agent_path = [env.agent_pos.copy()]
    goal_pos = env.goal_pos.copy()
    # ìž¥ì• ë¬¼ì€ ê³ ì •ì´ë¼ë©´ reset ì‹œì  ë³µì‚¬, (ì›€ì§ì´ëŠ” ìž¥ì• ë¬¼ë¡œ ë°”ê¾¸ë©´ env.obstaclesë¥¼ ë§¤ í”„ë ˆìž„ ì°¸ì¡°)
    obstacles = env.obstacles.copy() if hasattr(env, "obstacles") and env.obstacles is not None else np.zeros((0, 2), dtype=np.float32)

    def world_to_screen(pos):
        return int(size[0] / 2 + pos[0] * scale), int(size[1] / 2 - pos[1] * scale)

    done = False
    while running and not done:
        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                running = False

        with torch.no_grad():
            action, _ = actor.sample(state_tensor)
        action = action.cpu().numpy()[0]
        state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        agent_path.append(env.agent_pos.copy())

        # ìƒíƒœ ì—…ë°ì´íŠ¸(ë°°ìš° ìž…ë ¥ ì°¨ì›ì— ë§žì¶¤)
        state = adapt_state_for_actor(state, expected_dim)
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)

        # --- ì‹œê°í™” ---
        screen.fill((255, 255, 255))

        # ì´ë™ ê²½ë¡œ
        if len(agent_path) > 1:
            for i in range(1, len(agent_path)):
                pygame.draw.line(
                    screen, (0, 150, 255),
                    world_to_screen(agent_path[i - 1]),
                    world_to_screen(agent_path[i]), 2
                )

        # ëª©í‘œ/ì‹œìž‘/í˜„ìž¬
        pygame.draw.circle(screen, (255, 0, 0), world_to_screen(goal_pos), 8)          # ëª©í‘œ
        pygame.draw.circle(screen, (0, 255, 0), world_to_screen(agent_path[0]), 6)     # ì‹œìž‘
        pygame.draw.circle(screen, (0, 0, 255), world_to_screen(env.agent_pos), 6)     # í˜„ìž¬

        # ìž¥ì• ë¬¼(íšŒìƒ‰ ì›)
        if obstacles is not None and len(obstacles) > 0 and hasattr(env, "obstacle_radius"):
            r_px = max(1, int(env.obstacle_radius * scale))
            for obs in obstacles:
                pygame.draw.circle(screen, (128, 128, 128), world_to_screen(obs), r_px)

        pygame.display.flip()
        clock.tick(60)
        pygame.time.delay(wait)

    dist = np.linalg.norm(env.goal_pos - env.agent_pos)
    success = dist < getattr(env, "threshold", 0.25)

    if success:
        print(f"âœ” ëª©í‘œ ë„ë‹¬ ì„±ê³µ! ê±¸ë¦° ìŠ¤í… ìˆ˜: {env.steps} / {env.max_steps}")
    else:
        print(f"âœ˜ ëª©í‘œ ë„ë‹¬ ì‹¤íŒ¨. ìµœëŒ€ ìŠ¤í… ë„ë‹¬ ({env.max_steps}ìŠ¤í… ì‚¬ìš©)")

    if auto_quit:
        pygame.time.delay(1000)
        pygame.quit()

    return success, env.steps


def run_multiple_evaluations(model_path="sac_actor.pth", episodes=10):
    # ìž¥ì• ë¬¼ í¬í•¨ í™˜ê²½ìœ¼ë¡œ ìƒì„± (ì›í•˜ëŠ” ê°œìˆ˜/ë°˜ê²½ ì¡°ì ˆ)
    env = ENV.Vector2DEnv(map_range=12.8, step_size=0.1, num_obstacles=5, obstacle_radius=0.5)
    # ë§µ í¬ê¸°ì— ë”°ë¼ ìžë™ ìŠ¤ì¼€ì¼
    scale = 600 / (env.map_range * 2)

    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë°°ìš° ìž…ë ¥ ì°¨ì›ì„ ì¶”ë¡ í•˜ì—¬ ê·¸ì— ë§žê²Œ ë„¤íŠ¸ì›Œí¬ êµ¬ì„±
    sd = torch.load(model_path, map_location=device)
    exp_dim = infer_actor_input_dim_from_state_dict(sd)
    if exp_dim is None:
        # ì¶”ë¡  ì‹¤íŒ¨ ì‹œ í™˜ê²½ ìƒíƒœ ì°¨ì›ìœ¼ë¡œ ê°€ì •
        exp_dim = env.observation_space.shape[0]

    actor = Model.GaussianPolicy(state_dim=exp_dim, action_dim=env.action_space.shape[0]).to(device)

    # ê°€ì¤‘ì¹˜ ë¡œë“œ: êµ¬ì¡°ê°€ ì •í™•ížˆ ë§žìœ¼ë©´ strict=True, ì¼ë¶€ ë¶ˆì¼ì¹˜ ëŒ€ë¹„í•´ False
    try:
        actor.load_state_dict(sd, strict=True)
    except Exception as e:
        print(f"[warn] strict=True ë¡œë“œ ì‹¤íŒ¨: {e}\n â†’ strict=Falseë¡œ ìž¬ì‹œë„í•©ë‹ˆë‹¤.")
        actor.load_state_dict(sd, strict=False)

    actor.eval()

    success_count = 0
    total_steps = 0

    for i in range(episodes):
        print(f"\nðŸŒŸ ì—í”¼ì†Œë“œ {i + 1} ì‹œìž‘")
        success, steps = evaluate(env, actor, scale=scale, wait=10, auto_quit=(i == episodes - 1))
        success_count += int(success)
        total_steps += steps

    print("\nðŸ“Š í‰ê°€ ìš”ì•½:")
    print(f"- ì´ ì—í”¼ì†Œë“œ ìˆ˜: {episodes}")
    print(f"- ì„±ê³µë¥ : {success_count / episodes * 100:.2f}%")
    print(f"- í‰ê·  ìŠ¤í… ìˆ˜: {total_steps / episodes:.2f}")


if __name__ == "__main__":
    model_path = sys.argv[1] if len(sys.argv) > 1 else "sac_actor.pth"
    run_multiple_evaluations(model_path=model_path, episodes=10)
